{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..') #to add top-level to path\n",
    "sys.path.append('../modules') #to add top-level to path\n",
    "\n",
    "from modules.project_helper import TweetData\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = TweetData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_tweets = tweet_data.clean_tweets #[pd.to_datetime(tweet_data.clean_tweets.after4_date)\n",
    "                                       #<= pd.to_datetime(daily_df.index[-1])]\n",
    "daily_tweets.after4_date = pd.to_datetime(daily_tweets.after4_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>after4_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-17 19:57:12-06:00</th>\n",
       "      <td>tell jennifer williams whoever that is to read...</td>\n",
       "      <td>2019-11-17 19:57:12-06:00</td>\n",
       "      <td>2019-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17 19:56:02-06:00</th>\n",
       "      <td></td>\n",
       "      <td>2019-11-17 19:56:02-06:00</td>\n",
       "      <td>2019-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17 19:49:47-06:00</th>\n",
       "      <td>paul krugman of has been wrong about me from t...</td>\n",
       "      <td>2019-11-17 19:49:47-06:00</td>\n",
       "      <td>2019-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17 19:47:32-06:00</th>\n",
       "      <td>schiff is a corrupt politician</td>\n",
       "      <td>2019-11-17 19:47:32-06:00</td>\n",
       "      <td>2019-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-17 19:30:09-06:00</th>\n",
       "      <td>blew the nasty amp obnoxious chris wallace wil...</td>\n",
       "      <td>2019-11-17 19:30:09-06:00</td>\n",
       "      <td>2019-11-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      tweets  \\\n",
       "timestamp                                                                      \n",
       "2019-11-17 19:57:12-06:00  tell jennifer williams whoever that is to read...   \n",
       "2019-11-17 19:56:02-06:00                                                      \n",
       "2019-11-17 19:49:47-06:00  paul krugman of has been wrong about me from t...   \n",
       "2019-11-17 19:47:32-06:00                    schiff is a corrupt politician    \n",
       "2019-11-17 19:30:09-06:00  blew the nasty amp obnoxious chris wallace wil...   \n",
       "\n",
       "                                          timestamp after4_date  \n",
       "timestamp                                                        \n",
       "2019-11-17 19:57:12-06:00 2019-11-17 19:57:12-06:00  2019-11-18  \n",
       "2019-11-17 19:56:02-06:00 2019-11-17 19:56:02-06:00  2019-11-18  \n",
       "2019-11-17 19:49:47-06:00 2019-11-17 19:49:47-06:00  2019-11-18  \n",
       "2019-11-17 19:47:32-06:00 2019-11-17 19:47:32-06:00  2019-11-18  \n",
       "2019-11-17 19:30:09-06:00 2019-11-17 19:30:09-06:00  2019-11-18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/tweet_embeddings/trump_rnn_1911.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.in_embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4574, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump_rnn_1702.net',\n",
       " 'trump_rnn_1703.net',\n",
       " 'trump_rnn_1704.net',\n",
       " 'trump_rnn_1705.net',\n",
       " 'trump_rnn_1706.net']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arches = os.listdir('../models/tweet_embeddings')\n",
    "arches[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "for arch in arches:\n",
    "    model = torch.load('../models/tweet_embeddings/{}'.format(arch))\n",
    "    embedding_list.append(model.in_embed.weight.to('cpu').data.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2576, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TweetData('../data/intermediate_data/trump_archive_ts/trump_archive_db_1911.csv')\n",
    "words = data.words\n",
    "vocab_to_int, int_to_vocab = data.vocab_to_int, data.int_to_vocab\n",
    "int_words = data.int_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08305283, -0.16442633, -0.07570944, ..., -0.0107758 ,\n",
       "         0.19031201, -0.08310907],\n",
       "       [-0.18238883, -0.13035859,  0.12322256, ..., -0.1847221 ,\n",
       "         0.1905269 , -0.1955786 ],\n",
       "       [-0.22082756,  0.07478237, -0.33069697, ..., -0.14277695,\n",
       "         0.07682032,  0.05820784],\n",
       "       ...,\n",
       "       [ 0.9485307 , -0.02588199,  0.28236574, ..., -0.3737706 ,\n",
       "         0.12231396,  0.58168954],\n",
       "       [-0.3112953 , -0.4661302 ,  0.3573344 , ...,  0.6376106 ,\n",
       "        -0.22513622, -0.07564565],\n",
       "       [-0.71132535,  1.004769  , -0.1239484 , ..., -0.73008114,\n",
       "         0.34569532, -0.20363104]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2019-11-17 19:57:12-06:00   2019-11-18\n",
       "2019-11-17 19:56:02-06:00   2019-11-18\n",
       "2019-11-17 19:49:47-06:00   2019-11-18\n",
       "2019-11-17 19:47:32-06:00   2019-11-18\n",
       "2019-11-17 19:30:09-06:00   2019-11-18\n",
       "                               ...    \n",
       "2009-05-12 14:07:28-05:00   2009-05-12\n",
       "2009-05-08 20:40:15-05:00   2009-05-09\n",
       "2009-05-08 13:38:08-05:00   2009-05-08\n",
       "2009-05-05 01:00:10-05:00   2009-05-05\n",
       "2009-05-04 18:54:02-05:00   2009-05-05\n",
       "Name: after4_date, Length: 28813, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_tweets.after4_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_embeddings = {}\n",
    "dtws = daily_tweets.tweets[daily_tweets.after4_date >= pd.to_datetime('1-1-2017')]\n",
    "tweet_embeddings_np = np.zeros([dtws.shape[0],100])\n",
    "\n",
    "for i, tweet in enumerate(dtws):\n",
    "    embed_sum = np.zeros(100) \n",
    "    tw = tweet.split()\n",
    "    wd_count = 0\n",
    "    for word in tw:\n",
    "        try:\n",
    "            if vocab_to_int[word] >= 25:\n",
    "                embed_sum += embedding_list[-1][vocab_to_int[word]]\n",
    "                wd_count += 1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    if wd_count > 0:    \n",
    "        tweet_embeddings[i] = embed_sum/wd_count\n",
    "        tweet_embeddings_np[i,:] = embed_sum/wd_count\n",
    "    else:\n",
    "        tweet_embeddings[i] = embed_sum\n",
    "        tweet_embeddings_np[i,:] = embed_sum\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9582, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_embeddings_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tweet_embeddings_np).to_csv('../data/intermediate_data/tweet_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collusion\n",
      "russia\n",
      "campaign\n",
      "russians\n",
      "when\n",
      "democrats\n",
      "investigation\n",
      "fisa\n",
      "fbi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'democrats wrote to the ukrainian government in may urging it to continue investigations into president donald trump s alleged collusion with russia in the presidential campaign collusion later found not to exist '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = 1000\n",
    "\n",
    "embedding = model.in_embed\n",
    "embed_vectors = embedding.weight\n",
    "magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "tweet_after = daily_tweets.tweets[daily_tweets.after4_date >= pd.to_datetime('1-1-2017')]\n",
    "valid_vector = torch.FloatTensor(tweet_embeddings[tw])\n",
    "wds = (torch.matmul(valid_vector, embed_vectors.t()) / torch.FloatTensor(magnitudes)).topk(10)\n",
    "for wd in list(wds[1][0].numpy()):\n",
    "    if wd> 25 :\n",
    "        print(int_to_vocab[wd])\n",
    "tweet_after[tw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.9849, 0.9564, 0.9179, 0.8600, 0.8480, 0.8418, 0.8298, 0.8290, 0.8268,\n",
       "         0.8186]], grad_fn=<TopkBackward>),\n",
       "indices=tensor([[ 206,  178,  162, 1150,   87,   77,  596, 1592,   12,  223]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[1.4677, 1.2926, 1.2896, 1.2749, 1.2671, 1.2668, 1.2552, 1.2538, 1.2535,\n",
      "         1.2416]]),\n",
      "indices=tensor([[1467, 2024, 2254,  289, 1203,  878, 1575, 1879,  290, 2399]]))\n",
      "the federal reserve loves watching our manufacturers struggle with their exports to the benefit of other parts of the world has anyone looked at what almost all other countries are doing to take advantage of the good old usa our fed has been calling it wrong for too long \n",
      "\n",
      "with almost no inflation our country is needlessly being forced to pay a much higher interest rate than other countries only because of a very misguided federal reserve in addition quantitative tightening is continuing making it harder for our country to compete as good \n",
      "\n",
      "strong jobs report low inflation and other countries around the world doing anything possible to take advantage of the united states knowing that our federal reserve doesn t have a clue they raised rates too soon too often amp tightened while others did just the opposite \n",
      "\n",
      "our manufacturers we should have lower interest rates than germany japan and all others we are now by far the biggest and strongest country but the fed puts us at a competitive disadvantage china is not our problem the federal reserve is we will win anyway \n",
      "\n",
      "the usa should always be paying the the lowest rate no inflation it is only the na vet of jay powell and the federal reserve that doesn t allow us to do what other countries are already doing a once in a lifetime opportunity that we are missing because of boneheads \n",
      "\n",
      "as i predicted jay powell and the federal reserve have allowed the dollar to get so strong especially relative to all other currencies that our manufacturers are being negatively affected fed rate too high they are their own worst enemies they don t have a clue pathetic \n",
      "\n",
      "we are competing with many countries that have a far lower interest rate and we should be lower than them yesterday highest dollar in u s history no inflation wake up federal reserve such growth potential almost like never before \n",
      "\n",
      "countries that know how to play the game against the u s that s actually why the e u was formed and for china until now the u s has been easy pickens the fed has made all of the wrong moves a small rate cut is not enough but we will win anyway \n",
      "\n",
      "people are very disappointed in jay powell and the federal reserve the fed has called it wrong from the beginning too fast too slow they even tightened in the beginning others are running circles around them and laughing all the way to the bank dollar amp rates are hurting \n",
      "\n",
      "china gets of its oil from the straight japan amp many other countries likewise so why are we protecting the shipping lanes for other countries many years for zero compensation all of these countries should be protecting their own ships on what has always been \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tw = 1467\n",
    "\n",
    "embedding = model.in_embed\n",
    "embed_vectors = torch.FloatTensor(tweet_embeddings_np)\n",
    "magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "tweet_after = daily_tweets.tweets[daily_tweets.after4_date >= pd.to_datetime('1-1-2017')]\n",
    "valid_vector = torch.FloatTensor(tweet_embeddings[tw])\n",
    "wds = (torch.matmul(valid_vector, embed_vectors.t()) / torch.FloatTensor(magnitudes))\n",
    "wds[wds != wds] = 0\n",
    "print(wds.topk(10))\n",
    "for i in wds.topk(10).indices.numpy():\n",
    "    for tweet_range in i:\n",
    "        print(tweet_after[tweet_range])\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my daughter ivanka will be on tonight on at p m following the great at enjoy '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweet_after[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my daughter ivanka has been treated so unfairly by she is a great person always pushing me to do the right thing terrible '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_after[9335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'will be interviewed tonight by on at p m eastern enjoy '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_after[4955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will be interviewed live tonight by on p m enjoy '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_after[2434]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
